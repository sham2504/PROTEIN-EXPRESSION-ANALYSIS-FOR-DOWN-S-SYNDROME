# -*- coding: utf-8 -*-
"""PROTEIN EXPRESSION ANALYSIS FOR DOWN'S SYNDROME.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuoxS2678jB0fbPOVWM-twxoLn-NGCGw

## **IMPORTING LIBRARIES AND LOADING DATASET**
"""

from sklearn import feature_selection
import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn as sk
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from sklearn import model_selection
from sklearn.metrics import confusion_matrix,classification_report,roc_curve,f1_score,roc_auc_score,auc
from sklearn. preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectKBest,f_classif
import seaborn as sns
from scipy import interp
from itertools import cycle

mice=pd.read_csv("Data_Cortex_Nuclear.csv")

"""**THE DATASET CONSISTS OF 1080 ROWS AND 82 COLUMNS **"""

mice.shape

"""**OVERVIEW OF THE DATASET**"""

mice.head(10)

"""**VISUALIZING NULL VALUES IN THE DATASET**"""

sns.heatmap(mice.isnull())

"""**CHECKING FOR NULL VALUES AND FILLING THEM WITH THE MEAN VALUES **"""

mice.isnull().sum(axis=1)

nmice = mice.fillna(mice.mean())

nmice.isnull().sum()

"""**IDENTIFYING CATEGORICAL AND NUMERICAL ATTRIBUTES**"""

def value_type(mice):
  categorical=[]
  numerical=[]
  for i in mice.columns:
    if mice[i].dtype == 'object':
      categorical.append(i)
    else:
      numerical.append(i)
  return categorical,numerical

category,numerical=value_type(mice)
print('columns with categorical values:',category)
print('columns with numerical values:\n',numerical)

"""**IDENTIFICATION OF VALUES IN THE CATEGORICAL COLUMNS**

**CONTROL--NORMAL MICE**

**TS65DN--TRISOMIC OR DOWN SYNDROME AFFECTED MICE **
"""

print('values in Genotype column:',mice['Genotype'].nunique())
print('values:',mice['Genotype'].unique())

"""**MEMANTINE--MICE INJECTED WITH THE DRUG MEMANTINE**

**SALINE--MICE INJECTED WITH SALINE WATER **
"""

print('values in Treatment column:',mice['Treatment'].nunique())
print('values:',mice['Treatment'].unique())

"""**C/S--SIMULATED TO CONTEXT SHOCK**

**S/C--SIMULATED TO SHOCK CONTEXT **
"""

print('values in Behavior column:',mice['Behavior'].nunique())
print('values:',mice['Behavior'].unique())

"""C-CS-M-->CONTROL-CONTEXTSHOCK-MEMANTINE

C-SC-M-->CONTROL-SHOCKCONTEXT-MEMANTINE

C-CS-S-->CONTROL-CONTEXTSHOCK-SALINE

C-SC-S-->CONTROL-SHOCKCONTEXT-SALINE

T-CS-M-->TRISOMIC-CONTROLSHOCK-MEMANTINE

T-SC-M-->TRISOMIC-SHOCKCONTEXT-MEMANTINE

T-CS-S-->TRISOMIC-CONTROLSHOCK-SALINE

T-SC-S-->TRISOMIC-SHOCKCONTEXT-SALINE
"""

print('values in class column:',mice['class'].nunique())
print('values:',mice['class'].unique())

"""**ENCODING CATEGORICAL ATTRIBUTES TO NUMERICAL**"""

from sklearn import preprocessing
val=preprocessing.LabelEncoder()
nmice['Genotype']= val.fit_transform(nmice['Genotype'])
nmice['Treatment']= val.fit_transform(nmice['Treatment'])
nmice['Behavior']= val.fit_transform(nmice['Behavior'])
nmice

"""**IDENTIFICATION OF NUMBER OF RECORDS IN EACH CLASS**"""

sns.countplot(x='class',data=mice)

mice['class'].value_counts()

"""Control    570
Ts65Dn     510 
"""

sns.countplot(x="Genotype",data=mice)

"""S/C    555
C/S    525
"""

sns.countplot(x="Behavior",data=mice)

"""Memantine    570

Saline       510
"""

sns.countplot(x="Treatment",data=mice)

mice['Genotype'].value_counts().plot(kind='pie',autopct='%1.1f%%');

mice['class'].value_counts().plot(kind='pie',autopct='%1.1f%%');

ax=nmice["DYRK1A_N"].plot.hist()
ax.set_xlabel("DYRK1A_N")

ax=nmice["pPKCG_N"].plot.hist()
ax.set_xlabel("pPKCG_N")

ax=nmice["SOD1_N"].plot.hist()
ax.set_xlabel("SOD1_N")

ax=nmice["CaNA_N"].plot.hist()
ax.set_xlabel("SOD1_N")

ax=nmice["Ubiquitin_N"].plot.hist()
ax.set_xlabel("Ubiquitin_N")

sns.set(rc={"figure.figsize": (10,7)})
ax = sns.distplot(nmice.loc[:,'DYRK1A_N':'pERK_N'])
plt.show()

ax = sns.distplot(nmice.loc[:,'pJNK_N':'BRAF_N'])
plt.show()

ax = sns.distplot(X)
plt.show()

"""**THE PROTEIN VALUES IN THE DATASET ARE NORMALLY DISTRIBUTED**

**DIVIDING THE DATASET FOR TRAINING AND TESTING**
"""

X = nmice.drop(['class','Genotype','Treatment','Behavior','MouseID'],axis=1)
y = nmice['class']

"""**IDENTIFICATION OF THE MOST CONTRIBUTING PROTEINS TO THE OUTCOME**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
# instantiate SelectKBest to determine 20 best features
best_features = SelectKBest(score_func=f_classif, k=20)
fit = best_features.fit(X,y)
df_scores = pd.DataFrame(fit.scores_)
df_columns = pd.DataFrame(X.columns)
# concatenate dataframes
feature_scores = pd.concat([df_columns, df_scores],axis=1)
feature_scores.columns = ['Feature_Name','Score']  # name output columns
print(feature_scores.nlargest(20,'Score'))

"""THE PROTEINS CRITICAL TO LEARNING ABILITY ARE AS FOLLOWS


  SOD1_N

  CaNA_N 

  Ubiquitin_N  
        ARC_N  
        pS6_N  
        P38_N  
         S6_N   
     pPKCAB_N   
     pGSK3B_N   
       pERK_N   
      pMTOR_N   
       SNCA_N   
    pCAMKII_N   
        APP_N   
       IL1B_N   
      pNR2A_N   
      pNUMB_N   
     DYRK1A_N   
      ITSN1_N   
       MTOR_N

**TRAIN-TEST SPLITTING**
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35)

"""**DATA STANDARDIZATION**

**STANDARDIZING THE NUMERICAL ATTRIBUTES**
"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**MODEL BUILDING**

**K-NEAREST NEIGHBOR -- BEFORE HYPERPARAMETER TUNING**
"""

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
model_b= KNeighborsClassifier(n_neighbors = 6, metric = 'minkowski', p = 2)
model_b.fit(X_train, y_train)

"""**MODEL EVALUATION**"""

def multiclass_roc_auc_score(y_test, y_pred2, average="macro"):
  lb = preprocessing.LabelBinarizer()
  lb.fit(y_test)
  lb.fit(y_pred2)
  y_test = lb.transform(y_test)
  y_pred2 = lb.transform(y_pred2)
  return roc_auc_score(y_test, y_pred2, average=average)

y_pred = model_b.predict(X_test)
kfold = model_selection.KFold(n_splits=10, random_state = 0)
result = model_selection.cross_val_score(model_b, X_train, y_train, cv=kfold)
print('Accuracy of k-NN Model = ',result.mean())
f1_knn = f1_score(y_test, y_pred,average='macro')
print('f1 score:',f1_knn)
print('confusion matrix for 8  classes:\n',confusion_matrix(y_test,y_pred))

print("roc score:")
multiclass_roc_auc_score(y_test, y_pred)

"""**HYPER PARAMETER TUNING USING GRIDSEARCH CV**"""

#making the instance
model = KNeighborsClassifier()
#Hyper Parameters Set
params = {'n_neighbors':[5,6,7,8,9,10],
          'leaf_size':[1,2,3,5,6,7,8,9,10],
          'weights':['uniform', 'distance'],
          'algorithm':['auto', 'ball_tree','kd_tree','brute'],
          'n_jobs':[-1,1,2]}
#Making model with hyper parameters sets
model1 = GridSearchCV(model, param_grid=params, n_jobs=1)
#Learning
model1.fit(X_train,y_train)
#The best hyper parameters set
print("Best Hyper Parameters:\n",model1.best_params_)

"""**KNN MODEL WITH BEST HYPER PARAMETERS**"""

model_a= KNeighborsClassifier(n_neighbors = 6, metric = 'minkowski',leaf_size=1,n_jobs=-1,weights='distance')
model_a.fit(X_train, y_train)

"""**PREDICTION AND EVALUATION USING K FOLD CROSS VALIDATION**"""

y_pred1 = model_a.predict(X_test)
kfold = model_selection.KFold(n_splits=10)
result = model_selection.cross_val_score(model_a, X_train, y_train, cv=kfold)
y_pred1

print('Accuracy of k-NN Model = ',result.mean())
f1_knn = f1_score(y_test, y_pred1,average='macro')
print('f1 score:',f1_knn)
print('confusion matrix for 8  classes:\n',confusion_matrix(y_test,y_pred1))

print("roc score:")
multiclass_roc_auc_score(y_test, y_pred1)

"""GIVEN THE PROTEIN EXPRESSION VALUES OF THE SPECIFIED 77 PROTEINS, THE K NEAREST NEIGHBOUR MODEL PREDICTS THE TYPE OF MOUSE WHETHER ITS GENOTYPE IS CONTROL OR TRISOMIC, TREATMENT GIVEN IS MEMANTINE OR SALINE AND BEHAVIOR IS CONTEXT SHOCK OR SHOCK CONTEXT WITH AN ACCURACY OF 96.44%.

**PREDICTING FOR UNSEEN DATA**
"""

new_data1=[['0.503644','2.816329','5.990152','0.218830','0.177565','1.750936','0.687906','0.306382','0.402698','1.022060',
            '1.877684','0.441599','0.859366','0.416289','1.866358','3.685247','0.165846','3.037621','0.369510','0.458539',
            '0.335336','0.576916','0.448099','0.586271','0.394721','0.339571','0.482864','0.114783','0.131790','0.128186', 
            '1.443091','0.294700','0.354605','1.339070','0.188852','0.106305','0.144989','0.176668','0.125190','0.142756',
             '0.430957','1.603310','2.014875','0.108234','0.134762','0.427099','0.114783','0.131790','0.128186','0.9964',
            '0.503644','0.747193','0.430175','2.816329','5.990152','0.218830','0.177565','2.373744','0.232224','1.750936','0.687906',
            '0.306382','0.402698','0.296927','1.022060','0.605673','1.877684','2.308745','0.441599','0.859366','0.416289','0.369608',
            '0.178944','1.866358','3.685247','1.537227','0.264526'	
          ]]
a=model_a.predict(new_data1)
print("the given input data belongs to class:",a)

model1=LogisticRegression(C=10, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='multinomial', n_jobs=None, penalty='l2', random_state=None,
          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)
model1.fit(X_train, y_train)
y_pred4 = model1.predict(X_test)

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(n_estimators = 15, max_depth=None,criterion = 'entropy', random_state = 0)
model2.fit(X_train, y_train)
y_pred5 = model2.predict(X_test)

"""**VOTING ENSEMBLE CLASSIFIER**"""

models = list()
models.append(('knn',model_a))
models.append(('lr',model1))
models.append(('rf',model2))
ensemble = VotingClassifier(estimators=models, voting='hard')

ensemble.fit(X_train, y_train)
y_pred6 = ensemble.predict(X_test)
y_pred6

kfold = model_selection.KFold(n_splits=10)
results = model_selection.cross_val_score(ensemble, X_train, y_train, cv=kfold)
print('accuracy score of voting classifier:',results.mean())
f1_VC = f1_score(y_test, y_pred6,average='macro')
print('f1 score:',f1_VC)
print('confusion matrix for 8  classes:\n',confusion_matrix(y_test,y_pred6))

print("roc score:")
multiclass_roc_auc_score(y_test, y_pred6)